{
  "hash": "83f070e0340121c83a222c5023b468bd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Implement Bayesian Inference using Stan in R\"\ndescription: \"Here, Stan is a probabilistic programming language written in C++ not the song by Eminem that I really like.\"\nauthor:\n  - name: Kaixing Liu\ndate: 08-08-2024\ncategories: [Bayesian Statistics, Stan, R] # self-defined categories\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nformat:\n  html:\n    code-fold: true\n    code-copy: true\n---\n\n\n\n\n\nUnder the Bayesian paradigm, the statistical inference is conducted on the posterior distribution of parameters or the posterior predictive distribution of outcomes depending on the quantity of interest. In most cases, the posterior distributions are complex and known up to a constant factor (unknown) at best. It is Markov Chain Monte Carlo (MCMC) that makes the posterior estimation/inference doable and easier via simulating draws from the posterior distribution.\n\nToday, how MCMC works is not the focus. Instead, I will use several examples to illustrate conducting Bayesian inference with Stan (based on Hamiltonian MCMC) in R.\n\nFirst of all, we should install `rstan` package, the R interface to Stan, and the tools for compilation of C++ code in our computer. ([Refer to Stan official guide for details.](https://mc-stan.org/users/interfaces/rstan.html))\n\n### Example 1, beta-binomial\n\n$$\n\\theta \\sim \\text{Beta}(\\alpha,\\beta)\n$$\n\n$$\nY|\\theta \\sim \\text{Bernoulli}(\\theta)\n$$ Observed data, $\\{y_1,y_2,...,y_n\\}$ (**iid**).\n\n$$\np(\\theta|\\boldsymbol{y})\\propto p(\\boldsymbol{y}|\\theta)\\pi(\\theta)\\propto \\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n$$\n\nTherefore,\n\n$$\n\\theta|\\boldsymbol{y}\\sim \\text{Beta}(\\alpha+\\sum_i y_i,\\beta+n-\\sum_i y_i)\n$$\n\n::: {.callout-note appearance=\"simple\"}\nThis posterior distribution is a well-known distribution and we don't bother to use Stan; it is chosen mainly to have theoretical results to compare with those obtained from Stan.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify prior and data generation mechanism\n## hyperparameters\nalpha = 1\nbeta = 1 ### thus an uninformative prior\n##\ntheta = 0.45\nn = 100\nset.seed(0808)\ny = rbinom(n, 1, prob = theta)\n```\n:::\n\n\n\n\n\nBefore we call Stan in R, we need to create a `.stan` file (easy to do in Rstudio) written in Stan language, pretty straightforward and concise. ([Stan Reference Manual for language details](https://mc-stan.org/docs/reference-manual/))\n\n\n\n\n\n::: {.cell output.var='beta_binomial_model'}\n\n```{.stan .cell-code}\ndata { // Y is the observation vector with N observations\n  int<lower=0> N;\n  array[N] int Y;\n}\n\nparameters { // theta is bounded between 0 and 1\n  real<lower=0, upper=1> theta;\n}\n\nmodel {\n  theta ~ beta(1, 1); // specify prior distribution\n  for(n in 1:N){\n    Y[n] ~ binomial(1, theta); // specify model for data\n  }\n}\n\n```\n:::\n\n\n\n\n\nNow we can load `rstan` package and call `stan` function to simulate a posterior sample. (For this example, executable Stan code is inserted to output a `stanmodel` for `sampling` function as input. If we create a separate `.stan` file in the same directory, use `stan` function.)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores()) # parallel computing, one chain per core\nrstan_options(auto_write = TRUE) # only need one time of compilation of C++ code\ndat = list(N=n, Y=y) # consistent with data block in .stan file\nbeta_binomial_fit = sampling(beta_binomial_model, data = dat, algorithm=\"HMC\")\n```\n:::\n\n\n\n\n\n::: {.callout-note appearance=\"simple\"}\nInitial points (values) for MCMC are likely to influence the time it takes for the chains to reach the stationary distribution (target posterior distribution). Thus, draws during sampling period (default, 1001-2000) of each chain (default, 4 chains) are collapsed together to form a final posterior sample.\n:::\n\nThe match between the histogram of the sample and the theoretical density curve of the posterior distribution is satisfactory.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta_post = extract(beta_binomial_fit, permuted=T)$theta\nhist(theta_post, freq = F, main = NULL, xlim = c(0,1),\n     xlab = \"theta|y\")\nlines(seq(0.1, 1, length.out = 1000), dbeta(seq(0.1, 1, length.out = 1000),\n                              shape1 = alpha+sum(y),\n                              shape2 = beta+n-sum(y)),\n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of theta in beta-binomial example](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Example 2, normal-normal\n\n$$\n\\mu\\sim N(\\mu_0, \\sigma_0^2)\n$$\n\n$$\nY|\\mu,\\sigma^2\\sim N(\\mu, \\sigma^2)\n$$\n\n$$\np(\\mu|\\boldsymbol y)\\propto \\exp(-\\frac{(\\mu-\\mu_0)^2}{2\\cdot \\sigma_0^2})\\exp(-\\frac{\\sum_i (y_i-\\mu)^2}{2\\cdot \\sigma^2})\n$$\n\nTherefore,\n\n$$\n\\mu|\\boldsymbol {y}\\sim N(\\frac{\\frac{1}{\\sigma_0^2}\\mu_0+\\frac{n}{\\sigma^2}\\bar y}{\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}},\\frac{\\sigma_0^2\\sigma^2}{n\\sigma_0^2+\\sigma^2})\n$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set prior and generate data\nmu_0 = 0\nsigma_0 = 1\nmu = 1.5\nset.seed(0808)\ny_normal = rnorm(n, mean = mu, sd = 5)\ndat_normal = list(N=n, Y=y_normal)\n```\n:::\n\n\n\n\n\n(Refer to [this](https://mc-stan.org/docs/functions-reference/) for functions defined in Stan)\n\n\n\n\n\n::: {.cell output.var='normal_normal_model'}\n\n```{.stan .cell-code}\ndata { // Y is the observation vector with N observations\n  int<lower=0> N;\n  array[N] real Y;\n}\n\nparameters { // mu is unbounded\n  real mu;\n}\n\nmodel {\n  target += normal_lpdf(mu|0, 1);\n  target += normal_lpdf(Y|mu, 5);\n}\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# MCMC and plot\nnormal_normal_fit = sampling(normal_normal_model, data = dat_normal, iter=2000, chains=4)\nmu_post = extract(normal_normal_fit, permuted=T)$mu\nhist(mu_post, freq = F, main = NULL,\n     xlab = \"mu|y\", xlim=c(-1,4))\nlines(seq(-1, 4, length.out = 1000), dnorm(seq(-1, 4, length.out = 1000),\n                              mean = (mu_0/sigma_0^2+sum(y_normal)/5^2)/(n/5^2+1/sigma_0^2),\n                              sd = sqrt(5^2*sigma_0^2/(n*sigma_0^2+5^2))),\n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of mu in normal-normal example](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Example 3, Dirichlet-multinomial\n\nThis is the multivariate-extension of beta-binomial model.\n\n$$\n(\\theta_1,\\theta_2,...,\\theta_K)\\sim \\text{Dir}(\\alpha_1,\\alpha_2,...,\\alpha_K),\\ \\text{where} \\sum_k \\theta_k=1\\ \\text{and}\\ \\theta_k\\geq 0\\ \\forall k \n$$\n\n$$\n(Y_1,Y_2,...,Y_K)\\sim \\text{Multinomial}(n,\\boldsymbol {\\theta}),\\ \\text{where} \\sum_i Y_i=n\n$$\n\n$$\np(\\boldsymbol{\\theta}|\\boldsymbol{y})\\propto \\Pi_k \\theta_k^{y_k}\\Pi_k \\theta_k^{\\alpha_k-1}\n$$\n\nTherefore,\n\n$$\n\\boldsymbol{\\theta}|\\boldsymbol{y}\\sim \\text{Dir}(\\alpha_1',\\alpha_2',...,\\alpha_K'), \\text{where}\\ \\alpha_k'=\\alpha_k+y_k\\ \\forall k \n$$\n\nIt [can be shown](https://statisticaloddsandends.wordpress.com/2021/04/20/marginal-distributions-of-the-dirichlet-distribution-and-the-aggregation-property/) that the marginal distribution of a Dirichlet distribution is a beta distribution, for example, $\\theta_1|\\boldsymbol {y}\\sim \\text{Beta}(\\alpha_1',\\sum_k\\alpha_k'-\\alpha_1')$.\n\nHere, we consider $5$ categories and a uniform prior with $\\alpha_k=1\\ \\forall k$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set prior and generate data\nAlpha = c(alpha1 = 1, alpha2 = 1, alpha3 = 1,\n          alpha4 = 1, alpha5 = 1)\nTheta = c(theta1 = 0.25, theta2 = 0.15, theta3 = 0.05,\n          theta4 = 0.35, theta5 = 0.20)\nset.seed(0808)\ny_multinomial = as.vector(rmultinom(1, n, prob = Theta))\n```\n:::\n\n::: {.cell output.var='dirichlet_multinomial_model'}\n\n```{.stan .cell-code}\ndata { // Y is the observation vector with K categories\n  int<lower=3> K;\n  array[K] int Y;\n}\n\nparameters { // Thetas are bounded and sum to 1 \n  simplex[K] Theta;\n}\n\nmodel {\n  Theta ~ dirichlet(rep_vector(1, K));\n  Y ~ multinomial(Theta);\n}\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# MCMC and plot\ndat_multinomial = list(K=5, Y=y_multinomial)\ndirichlet_multinomial_fit = sampling(dirichlet_multinomial_model,\n                                     data = dat_multinomial)\n\n##\nTheta1_post = unlist(extract(dirichlet_multinomial_fit, pars=\"Theta[1]\",\n                      permuted = T))\nhist(Theta1_post, freq = F, main = NULL, xlim = c(0,1),\n     xlab = \"theta[1]|y\")\nlines(seq(0, 1, length.out = 1000), dbeta(seq(0, 1, length.out = 1000),\n                              shape1 = Alpha[\"alpha1\"]+y_multinomial[1],\n                              shape2 = sum(Alpha[-1]+y_multinomial[-1])),\n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of thetas in dirichlet-multinomial example](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## \nTheta2_post = unlist(extract(dirichlet_multinomial_fit, pars=\"Theta[2]\",\n                      permuted = T))\nhist(Theta2_post, freq = F, main = NULL, xlim = c(0,1),\n     xlab = \"theta[2]|y\")\nlines(seq(0, 1, length.out = 1000), dbeta(seq(0, 1, length.out = 1000),\n                              shape1 = Alpha[\"alpha2\"]+y_multinomial[2],\n                              shape2 = sum(Alpha[-2]+y_multinomial[-2])),\n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of thetas in dirichlet-multinomial example](index_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\n##\nTheta3_post = unlist(extract(dirichlet_multinomial_fit, pars=\"Theta[3]\",\n                      permuted = T))\nhist(Theta3_post, freq = F, main = NULL, xlim = c(0,1),\n     xlab = \"theta[3]|y\")\nlines(seq(0, 1, length.out = 1000), dbeta(seq(0, 1, length.out = 1000),\n                              shape1 = Alpha[\"alpha3\"]+y_multinomial[3],\n                              shape2 = sum(Alpha[-3]+y_multinomial[-3])),\n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of thetas in dirichlet-multinomial example](index_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n\n```{.r .cell-code}\n##\nTheta4_post = unlist(extract(dirichlet_multinomial_fit, pars=\"Theta[4]\",\n                      permuted = T))\nhist(Theta4_post, freq = F, main = NULL, xlim = c(0,1),\n     xlab = \"theta[4]|y\")\nlines(seq(0, 1, length.out = 1000), dbeta(seq(0, 1, length.out = 1000),\n                              shape1 = Alpha[\"alpha4\"]+y_multinomial[4],\n                              shape2 = sum(Alpha[-4]+y_multinomial[-4])),\n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of thetas in dirichlet-multinomial example](index_files/figure-html/unnamed-chunk-10-4.png){width=672}\n:::\n:::\n\n\n\n\n\n### Example 4, Bayesian linear regression\n\nRefer to the Bayesian analysis of [classical regression]{.underline} in [Chapter 14 of Bayesian Data Analysis, 3rd](http://www.stat.columbia.edu/~gelman/book/BDA3.pdf).\n\n$$\np(\\boldsymbol{\\beta},\\sigma^2|X)\\propto \\sigma^{-2} \n$$\n\n$$\nY|\\boldsymbol{\\beta},\\sigma^2,X\\sim N(X\\boldsymbol{\\beta},\\sigma^2I),\\ \\text{where}\\ Y\\ \\text{is random vector of length n}\n$$\n\n$$\np(\\boldsymbol{\\beta},\\sigma^2|\\boldsymbol{y},X)\\propto \\sigma^{-n}\\exp(-\\frac{1}{2}(\\boldsymbol{y}-X\\boldsymbol{\\beta})^T(\\sigma^2I)^{-1}(\\boldsymbol{y}-X\\boldsymbol{\\beta}))\\sigma^{-2}\\\\\n\\propto \\sigma^{-(n+2)}\\exp(-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-X\\boldsymbol{\\beta})^T(\\boldsymbol{y}-X\\boldsymbol{\\beta}))\n$$\n\nWith some algebra,\n\n$$\n\\exp((\\boldsymbol{y}-X\\boldsymbol{\\beta})^T(\\boldsymbol{y}-X\\boldsymbol{\\beta}))\\propto \\exp(\\boldsymbol{\\beta}^TX^TX\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}^TX^Ty)\\\\\n\\propto \\exp((\\boldsymbol{\\beta}-(X^TX)^{-1}X^Ty)^T(X^TX)(\\boldsymbol{\\beta}-(X^TX)^{-1}X^Ty))\n$$\n\n$$\n\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y},X \\sim N((X^TX)^{-1}X^Ty, \\sigma^2(X^TX)^{-1})\n$$\n\n$$\np(\\boldsymbol{\\beta},\\sigma^2|\\boldsymbol{y},X)=p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y},X)p(\\sigma^2|\\boldsymbol{y},X)\n$$\n\nTherefore,\n\n$$\np(\\sigma^2|\\boldsymbol{y},X)=\\frac{\np(\\boldsymbol{\\beta},\\sigma^2|\\boldsymbol{y},X)}{p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y},X)}\\\\\n\\propto \\frac{\\sigma^{-(n+2)}\\exp(-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-X\\boldsymbol{\\beta})^T(\\boldsymbol{y}-X\\boldsymbol{\\beta}))}{\\sigma^{-p}\\exp(-\\frac{1}{2\\sigma^2}(\\boldsymbol{\\beta}-(X^TX)^{-1}X^Ty)^T(X^TX)(\\boldsymbol{\\beta}-(X^TX)^{-1}X^Ty)}\\\\\n\\propto \\sigma^{-(n+2-p)}\\exp(-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}^T\\boldsymbol{y}-(X^T\\boldsymbol{y})^T(X^TX)^{-1}X^T\\boldsymbol{y})) \n$$\n\nRefer to the pdf of [scaled inverse-chi-square distribution](https://en.wikipedia.org/wiki/Scaled_inverse_chi-squared_distribution),\n\n$$\np(\\sigma^2|\\boldsymbol{y},X)\\propto \\frac{1}{(\\sigma^2)^{\\frac{n-p}{2}+1}}\\exp(-\\frac{(n-p)\\frac{1}{n-p}(\\boldsymbol{y}-X\\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{y}-X\\hat{\\boldsymbol{\\beta}})}{2(\\sigma^2)})\n$$\n\nHence,\n\n$$\n\\sigma^2|\\boldsymbol{y},X\\sim \\text{scaled Inv-}\\chi^2(n-p,\\frac{1}{n-p}(\\boldsymbol{y}-X\\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{y}-X\\hat{\\boldsymbol{\\beta}}))\n$$\n\n::: {.callout-note appearance=\"simple\"}\n$p$ is the number of columns of the design matrix $X$ including a column of $1$. Recall that in frequentist classical linear regression, $\\hat{\\boldsymbol{\\beta}}=(X^TX)^{-1}X^T\\boldsymbol{y},\\ \\text{Var}(\\hat{\\boldsymbol{\\beta}})=\\sigma^2(X^TX)^{-1},\\ \\hat{\\sigma^2}= \\frac{1}{n-p}(\\boldsymbol{y}-X\\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{y}-X\\hat{\\boldsymbol{\\beta}})$.\n:::\n\nFor simplification, consider $p=2$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate data\nset.seed(0809)\nx = rbinom(n, 1, prob = 0.55) # covariate is binary\nbeta_0 = 0.5 # intercept\nbeta_1 = 0.2 # effect\nset.seed(0809)\ny_linear_regression = beta_0 + x*beta_1 + rnorm(n, 0, 1)\ndat_linear = list(N=n, Y=y_linear_regression, x0=rep(1, n), x1=x)\n```\n:::\n\n::: {.cell output.var='bayesian_linear_regression'}\n\n```{.stan .cell-code}\nfunctions {\n  real logposterior(real y, real beta0, real beta1, real x0, real x1, real sigma2){\n  return -(y-beta0*x0-beta1*x1)^2/(2*sigma2) - log(sigma2)/2;\n  }\n}\n\ndata { // Y is the outcome vector with N observations \n  int<lower=0> N;\n  array[N] real Y;\n  array[N] real x0;\n  array[N] real x1;\n}\n\nparameters { // beta and sigma2\n  real beta0;\n  real beta1;\n  real<lower=0> sigma2;\n}\n\n\nmodel {\n  target += -log(sigma2);\n  for(n in 1:N){\n    target += logposterior(Y[n], beta0, beta1, x0[n], x1[n], sigma2);\n  }\n}\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesian_linear_fit = sampling(bayesian_linear_regression,\n                               data=dat_linear)\nbeta0_post = extract(bayesian_linear_fit, par=\"beta0\", permuted=T)$beta0\nbeta1_post = extract(bayesian_linear_fit, par=\"beta1\", permuted=T)$beta1\nsigma2_post = extract(bayesian_linear_fit, par=\"sigma2\", permuted=T)$sigma2\nmean(beta0_post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4021409\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(beta1_post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.114133\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(sigma2_post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.213379\n```\n\n\n:::\n\n```{.r .cell-code}\nlm_fit = lm(y~x1, data = data.frame(y=dat_linear$Y, x1=dat_linear$x1))\ncoefficients(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x1 \n  0.4010346   0.1171188 \n```\n\n\n:::\n\n```{.r .cell-code}\nsum(lm_fit$residuals^2)/lm_fit$df.residual\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.191846\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# sigma2\nscaled_inv_chi2_pdf = function(x, nu, tau2){\n  return((((tau2*nu/2)^(nu/2))/(gamma(nu/2)))*(exp(-nu*tau2/(2*x))/(x^(1+nu/2))))\n}\nhist(sigma2_post, freq = F, main = NULL, xlim = c(0.5,2.5),\n     xlab = \"sigma2|y\")\nlines(seq(0.5, 2.5, length.out = 1000), scaled_inv_chi2_pdf(seq(0.5, 2.5, length.out = 1000), nu = lm_fit$df.residual, tau2 = sum(lm_fit$residuals^2)/lm_fit$df.residual),\n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of beta and sigma2 in bayesian linear regression](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# beta\nlibrary(MASS)\nX = matrix(c(dat_linear$x0, dat_linear$x1), ncol = 2, byrow = F)\nBeta = sapply(sigma2_post, function(z) mvrnorm(n=1, mu = solve(t(X)%*%X)%*%t(X)%*%as.matrix(dat_linear$Y, ncol=1), Sigma = z*solve(t(X)%*%X)))\n## beta0\nhist(beta0_post, freq = F, main = NULL, xlim = c(-0.5,1.5),\n     xlab = \"beta0|y\")\nlines(density(Beta[1, ])$x, density(Beta[1, ])$y, \n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of beta and sigma2 in bayesian linear regression](index_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code}\n## beta1\nhist(beta1_post, freq = F, main = NULL, xlim = c(-1,1.5),\n     xlab = \"beta1|y\")\nlines(density(Beta[2, ])$x, density(Beta[2, ])$y, \n      col=\"red\")\n```\n\n::: {.cell-output-display}\n![Posterior distribution of beta and sigma2 in bayesian linear regression](index_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n\n\n\n\n\n::: {.callout-tip appearance=\"simple\"}\nThe marginal posterior distribution of $\\boldsymbol \\beta$ is unknown up to this point and its conditional posterior distribution is known, multivariate normal. Thus, we can simulate posterior draws of $\\boldsymbol \\beta$ based on the conditional distribution and simulated draws of $\\sigma^2$ to compare with draws obtained from Stan.\n:::\n\n### Stan is Powerful!!!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}